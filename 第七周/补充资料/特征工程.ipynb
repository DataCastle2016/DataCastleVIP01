{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征工程\n",
    "特征处理是特征工程的核心部分，sklearn提供了较为完整的特征处理方法，包括数据预处理，特征选择，降维等。\n",
    "\n",
    "特征选择( Feature Selection )也称特征子集选择( Feature Subset Selection , FSS )，或属性选择( Attribute Selection )。是指从已有的M个特征(Feature)中选择N个特征使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以降低数据集维度的过程,是提高学习算法性能的一个重要手段,也是模式识别中关键的数据预处理步骤。对于一个学习算法来说,好的学习样本是训练模型的关键。\n",
    "\n",
    "此外，需要区分特征选择与特征提取。特征提取 ( Feature extraction )是指利用已有的特征计算出一个抽象程度更高的特征集，也指计算得到某个特征的算法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征处理\n",
    "####  数值型\n",
    "\n",
    "1. 幅度调整/归一化：python中会有一些函数比如preprocessing.MinMaxScaler()将幅度调整到 [0,1] 区间。\n",
    "\n",
    "2. 统计值：包括max, min, mean, std等。python中用pandas库序列化数据后，可以得到数据的统计值。\n",
    "\n",
    "3. 离散化：把连续值转成非线性数据。例如电商会有各种连续的价格表，从0.03到100元，假如以一元钱的间距分割成99个区间，用99维的向量代表每一个价格所处的区间，1.2元和1.6元的向量都是 [0,1,0,…,0]。pd.cut() 可以直接把数据分成若干段。\n",
    "\n",
    "4. 柱状分布：离散化后统计每个区间的个数做柱状图。\n",
    "\n",
    "#### 类别型\n",
    "类别型一般是文本信息，比如颜色是红色、黄色还是蓝色，我们存储数据的时候就需要先处理数据。处理方法有： \n",
    "\n",
    "1. one-hot编码，编码后得到哑变量。统计这个特征上有多少类，就设置几维的向量，pd.get_dummies()可以进行one-hot编码。 \n",
    "\n",
    "2. Hash编码成词向量\n",
    "\n",
    "3. Histogram映射：把每一列的特征拿出来，根据target内容做统计，把target中的每个内容对应的百分比填到对应的向量的位置。优点是把两个特征联系起来。\n",
    "\n",
    "#### 时间型\n",
    "时间型特征的用处特别大，既可以看做连续值（持续时间、间隔时间），也可以看做离散值（星期几、几月份）。\n",
    "\n",
    "　　连续值\n",
    "\n",
    "　　　　a) 持续时间(单页浏览时长)\n",
    "\n",
    "　　　　b) 间隔时间(上次购买/点击离现在的时间)\n",
    "\n",
    "　　离散值\n",
    "\n",
    "　　　　a) 一天中哪个时间段(hour_0-23)\n",
    "\n",
    "　　　　b) 一周中星期几(week_monday...)\n",
    "\n",
    "　　　　c) 一年中哪个星期\n",
    "\n",
    "　　　　d) 一年中哪个季度\n",
    "\n",
    "　　　　e) 工作日/周末\n",
    "\n",
    "　　数据挖掘中经常会用时间作为重要特征，比如电商可以分析节假日和购物的关系，一天中用户喜好的购物时间等。\n",
    "\n",
    "#### 组合特征\n",
    "1. 拼接型：简单的组合特征。例如挖掘用户对某种类型的喜爱，对用户和类型做拼接。正负权重，代表喜欢或不喜欢某种类型。 \n",
    "\n",
    "　　- user_id&&category: 10001&&女裙 10002&&男士牛仔 \n",
    "\n",
    "2. 模型特征组合： \n",
    "\n",
    "　　- 用GBDT产出特征组合路径 \n",
    "\n",
    "　　- 组合特征和原始特征一起放进LR训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征选择\n",
    "特征选择，就是从多个特征中，挑选出一些对结果预测最有用的特征。因为原始的特征中可能会有冗余和噪声。 \n",
    "\n",
    "特征选择和降维有什么区别呢？前者只踢掉原本特征里和结果预测关系不大的， 后者做特征的计算组合构成新特征。\n",
    "\n",
    "#### 过滤型\n",
    "　　 - 方法：  评估单个特征和结果值之间的相关程度， 排序留下Top相关的特征部分。 \n",
    "\n",
    "　　 - 评价方式：Pearson相关系数， 互信息， 距离相关度。 \n",
    "\n",
    "　　 - 缺点：只评估了单个特征对结果的影响，没有考虑到特征之间的关联作用， 可能把有用的关联特征误踢掉。因此工业界使用比较少。 \n",
    "\n",
    "　　 - python包：SelectKBest指定过滤个数、SelectPercentile指定过滤百分比。\n",
    "\n",
    "#### 包裹型\n",
    "\n",
    "　　- 方法：把特征选择看做一个特征子集搜索问题， 筛选各种特征子集， 用模型评估效果。 \n",
    "\n",
    "　　- 典型算法：“递归特征删除算法”。 \n",
    "\n",
    "　　- 应用在逻辑回归的过程：用全量特征跑一个模型；根据线性模型的系数(体现相关性)，删掉5-10%的弱特征，观察准确率/auc的变化；逐步进行， 直至准确率/auc出现大的下滑停止。 \n",
    "\n",
    "　　- python包：RFE \n",
    "　　\n",
    "\n",
    "#### 嵌入型\n",
    "\n",
    "　　 - 方法：根据模型来分析特征的重要性，最常见的方式为用正则化方式来做特征选择。 \n",
    "\n",
    "　　 - 举例：最早在电商用LR做CTR预估， 在3-5亿维的系数特征上用L1正则化的LR模型。上一篇介绍了L1正则化有截断作用，剩余2-3千万的feature， 意味着其他的feature重要度不够。 \n",
    "\n",
    "　　 - python包：feature_selection.SelectFromModel选出权重不为0的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更多详细的内容可以阅读\n",
    "\n",
    "https://www.zhihu.com/question/29316149"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA（主成分分析）\n",
    "PCA（Principal Components Analysis）即主成分分析。主成分分析也称主分量分析，旨在利用降维的思想，把多指标转化为少数几个综合指标。\n",
    "\n",
    "在统计学中，主成分分析PCA是一种简化数据集的技术。它是一个线性变换。这个变换把数据变换到一个新的坐标系统中，使得任何数据投影的第一大方差在第一个坐标(称为第一主成分)上，第二大方差在第二个坐标(第二主成分)上，依次类推。主成分分析经常用于减少数据集的维数，同时保持数据集的对方差贡献最大的特征。这是通过保留低阶主成分，忽略高阶主成分做到的。这样低阶成分往往能够保留住数据的最重要方面。但是，这也不是一定的，要视具体应用而定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用python中的sklearn，实现一个简单的实例：\n",
    "\n",
    "首先，训练集有6组数据，每组数据有4个特征，我们的目的是将其降到2维，也就是2个特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95713353 0.03398198]\n",
      "[[  7.96504337   4.12166867]\n",
      " [ -0.43650137   2.07052079]\n",
      " [-13.63653266   1.86686164]\n",
      " [-22.28361821  -2.32219188]\n",
      " [  3.47849303  -3.95193502]\n",
      " [ 24.91311585  -1.78492421]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "X = np.array([[-1,2,66,-1], [-2,6,58,-1], [-3,8,45,-2], [1,9,36,1], [2,10,62,1], [3,5,83,2]])  #导入数据，维度为4\n",
    "pca = PCA(n_components=2)   #降到2维\n",
    "pca.fit(X)                  #训练\n",
    "newX=pca.fit_transform(X)   #降维后的数据\n",
    "# PCA(copy=True, n_components=2, whiten=False)\n",
    "print(pca.explained_variance_ratio_)  #输出贡献率\n",
    "print(newX)                  #输出降维后的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数说明：\n",
    "\n",
    "1. n_components:  我们可以利用此参数设置想要的特征维度数目，可以是int型的数字，也可以是阈值百分比，如95%，让PCA类根据样本特征方差来降到合适的维数，也可以指定为string类型，MLE。\n",
    "2. copy： bool类型，TRUE或者FALSE，是否将原始数据复制一份，这样运行后原始数据值不会改变，默认为TRUE。\n",
    "3. whiten：bool类型，是否进行白化（就是对降维后的数据进行归一化，使方差为1），默认为FALSE。如果需要后续处理可以改为TRUE。\n",
    "4. explained_variance_： 代表降为后各主成分的方差值，方差值越大，表明越重要。\n",
    "5. explained_variance_ratio_： 代表各主成分的贡献率。\n",
    "6. inverse_transform()： 将降维后的数据转换成原始数据，X=pca.inverse_transform(newX)。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
